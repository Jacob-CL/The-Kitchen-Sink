{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "url = \"http://94.237.57.59:40857\"\n",
    "ip_address = \"94.237.57.59\"\n",
    "port = \"40857\"\n",
    "\n",
    "url2 = \"https://www.google.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4.1 Information Gathering](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/README)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4.1.1 Conduct Search Engine Discovery Reconnaissance for Information Leakage Notes](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/01-Conduct_Search_Engine_Discovery_Reconnaissance_for_Information_Leakage)\n",
    "- The goal is to identify what sensitive design and configuration information of the application, system, or organization is exposed directly (on the organization’s website) or indirectly (via third-party services).\n",
    "\n",
    "4.1.1 Notes\n",
    "\n",
    "The goal is to identify what sensitive design and configuration information of the application, system, or organization is exposed directly (on the organization’s website) or indirectly (via third-party services).\n",
    "\n",
    "- [Google Hacking Database](https://www.exploit-db.com/google-hacking-database)\n",
    "- [Google Hacking Diggity Project](https://resources.bishopfox.com/resources/tools/google-hacking-diggity/)\n",
    "- [Vulnerability Assessment.co.uk/PenetrationTest.html](vulnerabilityassessment.co.uk/Penetration%20Test.html)\n",
    "- [SecuritySift/passive-reconnaissance](securitysift.com/passive-reconnaissance/)\n",
    "- [PentestStandard.org/IntelligenceGathering](pentest-standard.org/index.php/Intelligence_Gathering)\n",
    "- [Onstrat.com/OSINT](onstrat.com/osint/)\n",
    "\n",
    "\n",
    "## Tools\n",
    "\n",
    "| Relationship and Recon Tools            | Name               |\n",
    "|-----------------------------------------|--------------------|\n",
    "| github.com/ElevenPaths/FOCA             | FOCA               |\n",
    "| github.com/laramies/theHarvester        | theHarvester       |\n",
    "| maltego.com                             | Maltego            |\n",
    "| https://github.com/lanmaster53/recon-ng | Recon-ng Framework |\n",
    "|                                         |                    |\n",
    "\n",
    "\n",
    "## Network\n",
    "\n",
    "| Network Resource                    | Website                               |\n",
    "|-------------------------------------|---------------------------------------|\n",
    "| DNSstuff Toolbox                    | [dnsstuff.com/tools](https://dnsstuff.com/tools) |\n",
    "| Network-Tools                       | [network-tools.com](http://network-tools.com) | \n",
    "| CentralOps                          | [centralops.net](http://centralops.net) |\n",
    "| Hurricane Electric                  | [lg.he.net](http://lg.he.net)         |\n",
    "| BGP                                 | [bgp4.as/looking-glasses](http://bgp4.as/looking-glasses) |\n",
    "| Shodan                              | [shodan.io](http://shodan.io)         |\n",
    "| GreyNoise                           | [viz.greynoise.io](http://viz.greynoise.io) |\n",
    "| MxToolBox                           | [mxtoolbox.com/NetworkTools.aspx](http://mxtoolbox.com/NetworkTools.aspx) |\n",
    "| IANA IP and ASN Lookup              | [iana.org/numbers](http://iana.org/numbers) |\n",
    "\n",
    "\n",
    "## WHOIS\n",
    "\n",
    "| WHOIS Resource | Website               |\n",
    "|----------------|-----------------------|\n",
    "| ICANN          | [icann.org](https://icann.org) |\n",
    "| IANA           | [iana.com](https://iana.com)   |\n",
    "| NRO            | [nro.net](https://nro.net)     |\n",
    "| AFRINIC        | [afrinic.net](https://afrinic.net) |\n",
    "| APNIC          | [apnic.net](https://apnic.net) |\n",
    "| ARIN           | [ws.arin.net](https://ws.arin.net) |\n",
    "| LACNIC         | [lacnic.net](https://lacnic.net) |\n",
    "| RIPE           | [ripe.net](https://ripe.net)   |\n",
    "| InterNIC       | [internic.net](https://internic.net) |\n",
    "\n",
    "## Search engines\n",
    "\n",
    "| Search Engine Name               | Description                                                                                           |\n",
    "|----------------------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| Baidu                            | China’s most popular search engine.                                                                   |\n",
    "| Bing                             | A search engine owned and operated by Microsoft, and the second most popular worldwide. Supports advanced search keywords. |\n",
    "| binsearch.info                   | A search engine for binary Usenet newsgroups.                                                         |\n",
    "| Common Crawl                     | \"An open repository of web crawl data that can be accessed and analyzed by anyone.\"                   |\n",
    "| DuckDuckGo                       | A privacy-focused search engine that compiles results from many different sources. Supports search syntax. |\n",
    "| Google                           | Offers the world’s most popular search engine, and uses a ranking system to attempt to return the most relevant results. Supports search operators. |\n",
    "| Internet Archive Wayback Machine | \"Building a digital library of Internet sites and other cultural artifacts in digital form.\"           |\n",
    "| Startpage                        | A search engine that uses Google’s results without collecting personal information through trackers and logs. Supports search operators. |\n",
    "| Shodan                           | A service for searching Internet-connected devices and services. Usage options include a limited free plan as well as paid subscription plans. |\n",
    "\n",
    "\n",
    "## Google Searching\n",
    "\n",
    "| Google Searches                        | Description                          |\n",
    "|----------------------------------------|--------------------------------------|\n",
    "| site:<URL>                             | Search only one                      |\n",
    "| numrange:<START_NUMBER>...<END_NUMBER> | Search within a number range         |\n",
    "| date:<INTEGER>                         | Search within past [#] months        |\n",
    "| link:<URL>                             | Find pages that link to given URL    |\n",
    "| related:<URL>                          | Find pages related to given URL      |\n",
    "| intitle:<STRING>                       | Find pages with <STRING> in title    |\n",
    "| inbody: / intext:                      | Only search for the keyword in the body of pages |\n",
    "| inurl:<STRING>                         | Find pages with <STRING> in URL      |\n",
    "| filetype:<EXTENSION>                   | Search for files by filetype         |\n",
    "| phonebook:<STRING>                     | Find phone book listings of <STRING> |\n",
    "| cache:<STRING>                         | Search for conent that has previously been indexed |\n",
    "|                                        |                                      |\n",
    "\n",
    "\n",
    "## People Searching Tools\n",
    "\n",
    "| Tool                           |                |\n",
    "|--------------------------------|----------------|\n",
    "| peekyou.com                    | PeekYou        | \n",
    "| spokeo.com                     | Spokeo         | \n",
    "| pipl.com                       | pipl.com       | \n",
    "| intelius.com                   | Intelius       | \n",
    "| publicrecords.searchsystem.net | Search Systems | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4.1.2 Fingerprint Web Server Notes](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/02-Fingerprint_Web_Server)\n",
    "- Determine the version and type of a running web server to enable further discovery of any known vulnerabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.2 CODE \n",
    "# Determine the version and type of a running web server to enable further discovery of any known vulnerabilities.\n",
    "\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def banner_grab(url):\n",
    "    # Parse the URL to extract the domain and possible port\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc.split(':')[0]  # Get the domain without the port\n",
    "    port = parsed_url.port if parsed_url.port else 80  # Use port 80 if no port specified\n",
    "\n",
    "    def send_request(request):\n",
    "        try:\n",
    "            # Create a socket object\n",
    "            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            s.settimeout(10)  # Set timeout for the socket\n",
    "            # Connect to the server\n",
    "            s.connect((domain, port))\n",
    "            # Send the HTTP request to the server\n",
    "            s.send(request.encode())\n",
    "            # Receive the response from the server\n",
    "            response = s.recv(4096)  # Adjust size as necessary\n",
    "            s.close()  # Always close the socket\n",
    "            # Decode and split the response at the first double newline, which ends the headers\n",
    "            headers, _, _ = response.decode().partition('\\r\\n\\r\\n')\n",
    "            return headers\n",
    "        except Exception as e:\n",
    "            return f\"Failed to connect or retrieve data: {e}\"\n",
    "        \n",
    "\n",
    "    # Standard HTTP GET request\n",
    "    normal_request = f\"GET / HTTP/1.1\\r\\nHost: {domain}\\r\\nConnection: close\\r\\n\\r\\n\"\n",
    "    headers = send_request(normal_request)\n",
    "    print(f\"\"\"\n",
    "--> BANNER GRABBING {url}\n",
    "================================================================================\n",
    "\n",
    "> Normal GET request sent:\\n{normal_request}\n",
    "> Normal GET request response:\\n{headers}\n",
    "--------------------------------------------------------------------------------\"\"\")\n",
    "\n",
    "\n",
    "    # Malformed GET request with non-existent method 'FAKE'\n",
    "    malformed_request = f\"GET / FAKE/1.1\\r\\nHost: {domain}\\r\\nConnection: close\\r\\n\\r\\n\"\n",
    "    headers = send_request(malformed_request)\n",
    "    print(f\"\"\"\n",
    "> Malformed GET request sent with non-existent protocol 'FAKE':\\n{malformed_request}\n",
    "> Malformed GET request reponse:\\n{headers}\n",
    "--------------------------------------------------------------------------------\"\"\")\n",
    "\n",
    "\n",
    "    # Malformed request with non-existent method 'FAKE'\n",
    "    malformed_request = f\"FAKE / HTTP/1.1\\r\\nHost: {domain}\\r\\nConnection: close\\r\\n\\r\\n\"\n",
    "    headers = send_request(malformed_request)\n",
    "    print(f\"\"\"\n",
    "> Malformed FAKE request sent with non-existent method 'FAKE':\\n{malformed_request}\n",
    "> Malformed FAKE request response:\\n{headers}\n",
    "--------------------------------------------------------------------------------\"\"\")\n",
    "    \n",
    "    # Different HTTP versions\n",
    "    malformed_request = f\"GET / HTTP/1.0\\r\\nHost: {domain}\\r\\nConnection: close\\r\\n\\r\\n\"\n",
    "    headers = send_request(malformed_request)\n",
    "    print(f\"\"\"\n",
    "> HTTP version 1.0 request sent:\\n{malformed_request}\n",
    "> HTTP version 1.0 request response:\\n{headers}\n",
    "--------------------------------------------------------------------------------\"\"\")\n",
    "    \n",
    "    # Different HTTP versions\n",
    "    malformed_request = f\"GET / HTTP/2.0\\r\\nHost: {domain}\\r\\nConnection: close\\r\\n\\r\\n\"\n",
    "    headers = send_request(malformed_request)\n",
    "    print(f\"\"\"\n",
    "> HTTP version 2.0 request sent:\\n{malformed_request}\n",
    "> HTTP version 2.0 request response:\\n{headers}\n",
    "--------------------------------------------------------------------------------\"\"\")\n",
    "    \n",
    "banner_grab(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4.1.3 Review Webserver Metafiles for Information Leakage Notes](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/03-Review_Webserver_Metafiles_for_Information_Leakage)\n",
    "- Identify hidden or obfuscated paths and functionality through the analysis of metadata files.\n",
    "- Extract and map other information that could lead to better understanding of the systems at hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.3 CODE\n",
    "# Identify hidden or obfuscated paths and functionality through the analysis of metadata files.\n",
    "# Extract and map other information that could lead to better understanding of the systems at hand.\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "\n",
    "# Function to check a URL and return its status code\n",
    "def get_url_status_code(url):\n",
    "    try:\n",
    "        response = requests.head(url, timeout=5)\n",
    "        return response.status_code\n",
    "    except requests.RequestException:\n",
    "        return None  # Return None if there was an error making the request\n",
    "\n",
    "print(\"\\n--> Enumerating URL by checking for all response codes less than 400...\\n\")\n",
    "\n",
    "# Hardcoded list of paths\n",
    "paths = [\n",
    "    '/',\n",
    "    '/nonexistentpage',\n",
    "    '/home',\n",
    "    '/about',\n",
    "    '/contact',\n",
    "    '/login',\n",
    "    '/search',\n",
    "    '/register',\n",
    "    '/signup',\n",
    "    '/blog',\n",
    "    '/faq',\n",
    "    '/products',\n",
    "    '/services',\n",
    "    '/profile',\n",
    "    '/settings',\n",
    "    '/terms',\n",
    "    '/help',\n",
    "    '/privacy',\n",
    "    '/account',\n",
    "    '/robots.txt',\n",
    "    '/index.html',\n",
    "    '/humans.txt',\n",
    "    '/sitemap.xml',\n",
    "    '/security.txt',\n",
    "    '/.well-known/security.txt',\n",
    "    '/.well-known/',\n",
    "    '/wp-includes/',\n",
    "    '/wp-admin/',\n",
    "    '/wp-content/',\n",
    "    '/login.php',\n",
    "    '/changelog.txt',\n",
    "    '/functions.php',\n",
    "]\n",
    "\n",
    "valid_urls = []\n",
    "# Iterate over the hardcoded paths and check each one\n",
    "for path in paths:\n",
    "    # Construct the full URL\n",
    "    extended_url = urljoin(url, path)\n",
    "\n",
    "    # Get the status code for the URL\n",
    "    status_code = get_url_status_code(extended_url)\n",
    "\n",
    "    # Determine if the URL is valid based on the status code\n",
    "    if status_code is not None and status_code < 400:\n",
    "        print(f\"{status_code} - Valid URL: {extended_url}\")\n",
    "        valid_urls.append(extended_url)  # Add the valid URL to the list\n",
    "    else:\n",
    "        # If status_code is None, it means the request failed\n",
    "        if status_code is None:\n",
    "            print(f\"Error making request to: {extended_url}\")\n",
    "        else:\n",
    "            print(f\"{status_code} ---- XX Invalid URL: {extended_url}\")\n",
    "\n",
    "print(f\"\\n✓✓ Enumeration on {url} complete. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4.1.4 Enumerate Applications on WebServer Notes](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/04-Enumerate_Applications_on_Webserver)\n",
    "- Enumerate the applications within scope that exist on a web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.4 CODE\n",
    "# Enumerate the applications within scope that exist on a web server, relatively manual process. Read link ^ \n",
    "import nmap\n",
    "import dns.resolver\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Create an Nmap PortScanner object\n",
    "nm = nmap.PortScanner()\n",
    "\n",
    "# List of common ports to scan\n",
    "common_ports = [\n",
    "    '21',    # FTP\n",
    "    '22',    # SSH\n",
    "    '23',    # Telnet\n",
    "    '25',    # SMTP\n",
    "    '53',    # DNS\n",
    "    '80',    # HTTP\n",
    "    '110',   # POP3\n",
    "    '143',   # IMAP\n",
    "    '443',   # HTTPS\n",
    "    '1433',  # MSSQL\n",
    "    '3306',  # MySQL\n",
    "    '3389',  # RDP\n",
    "    '8080',   # HTTP alternative\n",
    "    '40857'\n",
    "]\n",
    "\n",
    "try:\n",
    "    print(\"--> Performing NMAP scan on common ports with verbose output, returning only open ports..\\n\")\n",
    "    # Convert the list of common ports into a comma-separated string for Nmap\n",
    "    ports = ','.join(common_ports)\n",
    "    # Run the scan with service and version detection\n",
    "    nm.scan(ip_address, ports, arguments='-sV -O')  # Including service detection and OS detection\n",
    "\n",
    "    if nm.all_hosts():  # Check if any hosts were found\n",
    "        for host in nm.all_hosts():  # Loop through all hosts\n",
    "            print(f'Host : {host}')  # directly using `host` variable\n",
    "            print(f'State : {nm[host].state()}')\n",
    "\n",
    "            # Loop through all scanned protocols and corresponding ports\n",
    "            for proto in nm[host].all_protocols():\n",
    "                print('----------')\n",
    "                print(f'Protocol : {proto}')\n",
    "\n",
    "                lport = nm[host][proto].keys()\n",
    "                for port in sorted(lport):\n",
    "                    port_info = nm[host][proto][port]\n",
    "                    # Print detailed information for each port\n",
    "                    if port_info[\"state\"] == \"open\":\n",
    "                        print(f'Port : {port}\\tState : {port_info[\"state\"]}')\n",
    "                        if 'url' in port_info:\n",
    "                            print(f'\\tService : {port_info[\"url\"]}')\n",
    "                        if 'product' in port_info:\n",
    "                            print(f'\\tProduct : {port_info[\"product\"]}')\n",
    "                        if 'version' in port_info:\n",
    "                            print(f'\\tVersion : {port_info[\"version\"]}')\n",
    "                        if 'extrainfo' in port_info:\n",
    "                            print(f'\\tExtra Info : {port_info[\"extrainfo\"]}')\n",
    "                        if 'script' in port_info:\n",
    "                            print(f'\\tScripts : {port_info[\"script\"]}')\n",
    "    else:\n",
    "        print(\"No hosts found. Ensure the IP address or hosturl is correctly specified.\")\n",
    "except Exception as e:\n",
    "    print(f\"Scan error: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--> Quick scanning the first 1024 ports..\\n\")\n",
    "try:\n",
    "    # Run a simple Nmap scan\n",
    "    nm.scan(ip_address, '1-1024')  # Scans TCP ports 1 through 1024\n",
    "    if nm.all_hosts():  # Check if any hosts were found\n",
    "        for host in nm.all_hosts():  # Loop through all hosts\n",
    "            print('Host : %s (%s)' % (host, nm[host].hostname()))  # Get the host and its hostname\n",
    "            print('State : %s' % nm[host].state())  # Get the state of the host (up/down)\n",
    "\n",
    "            # Loop through all scanned protocols and corresponding ports\n",
    "            for proto in nm[host].all_protocols():\n",
    "                print('----------')\n",
    "                print('Protocol : %s' % proto)\n",
    "\n",
    "                lport = nm[host][proto].keys()\n",
    "                for port in sorted(lport):\n",
    "                    # Print port and state\n",
    "                    print('port : %s\\tstate : %s' % (port, nm[host][proto][port]['state']))\n",
    "    else:\n",
    "        print(\"No hosts found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Scan error: {e}\")\n",
    "\n",
    "print(\"\\n--> Trying for more DNS information..\\n\")\n",
    "# Get More Detailed DNS Information\n",
    "def dns_lookup(ip_address, record_type):\n",
    "    try:\n",
    "        # Perform the DNS query\n",
    "        response = dns.resolver.resolve(ip_address, record_type)\n",
    "        \n",
    "        # Print details about the DNS response\n",
    "        print(f\"Query for {ip_address} {record_type} records:\")\n",
    "        for answer in response:\n",
    "            print(f\"  - {record_type} record: {answer.to_text()}\")\n",
    "            print(f\"    TTL: {answer.ttl} seconds\")\n",
    "        \n",
    "        # Print additional response information\n",
    "        print(\"\\nAdditional response metadata:\")\n",
    "        print(f\"  Query time: {response.response.time * 1000:.2f} ms\")\n",
    "        print(f\"  ip_addressservers used for the query:\")\n",
    "        for ns in response.ip_addressservers:\n",
    "            print(f\"    {ns}\")\n",
    "        \n",
    "    except dns.resolver.NoAnswer:\n",
    "        print(f\"No {record_type} record found for {ip_address}\")\n",
    "    except dns.resolver.NXDOMAIN:\n",
    "        print(f\"The domain {ip_address} does not exist\")\n",
    "    except dns.resolver.Timeout:\n",
    "        print(\"The request timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "dns_lookup(ip_address, 'A')  # A record lookup\n",
    "dns_lookup(ip_address, 'MX')  # MX record lookup\n",
    "dns_lookup(ip_address, 'NS')  # NS record lookup\n",
    "\n",
    "def get_ip_address(url):\n",
    "    print(\"\\n--> Trying to get IP address..\\n\")\n",
    "    try:\n",
    "        # Extract the hostname from the URL\n",
    "        hostname = urlparse(url).hostname\n",
    "        # Resolve the hostname to an IP address\n",
    "        ip_address = socket.gethostbyname(hostname)\n",
    "        print(f\"The IP address of {url} is {ip_address}\")\n",
    "    except socket.error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "get_ip_address(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4.1.5 Review Webpage Content for Information Leakage Notes](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/05-Review_Webpage_Content_for_Information_Leakage)\n",
    "- Review webpage comments and metadata to find any information leakage.\n",
    "- Gather JavaScript files and review the JS code to better understand the application and to find any information leakage.\n",
    "- Identify if source map files or other front-end debug files exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.5 CODE\n",
    "# Review webpage comments and metadata to find any information leakage.\n",
    "# Gather JavaScript files and review the JS code to better understand the application and to find any information leakage.\n",
    "# Identify if source map files or other front-end debug files exist.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Define lists of search terms for different categories\n",
    "databases = ['MySQL', 'Mongo', 'MSSQL', 'noSQL', 'postgreSQL']\n",
    "frontend_filtering = ['dompurify', 'caja', 'sanitize', 'filter', 'addslashes', 'blacklist', 'black-list', 'black list', 'whitelist', 'white-list', 'white list']\n",
    "html_comments = ['<!--']\n",
    "html_version = ['<!DOCTYPE', '<head']\n",
    "html_inputs = ['<input', '<textarea', '<form']\n",
    "html_meta_tags = ['<meta']\n",
    "javascript_tags = ['<script']\n",
    "random_poi = ['.php', '.aspx', '.jsp']\n",
    "\n",
    "# Map these lists to their respective category names for easier management and output\n",
    "categories = {\n",
    "    \"Databases\": databases,\n",
    "    \"Frontend Filtering\": frontend_filtering,\n",
    "    \"HTML Comments\": html_comments,\n",
    "    \"HTML Version\": html_version,\n",
    "    \"HTML Inputs\": html_inputs,\n",
    "    \"HTML Meta Tags\": html_meta_tags,\n",
    "    \"JavaScript Tags\": javascript_tags,\n",
    "    \"Other random points of interest:\" : random_poi,\n",
    "}\n",
    "\n",
    "def static_analysis(url):\n",
    "    print(f\"\"\"STATIC SOURCE ANALYSIS ON {url}\n",
    "================================================================================\"\"\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "        content_lines = response.text.splitlines()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Iterate over each category and its associated list of search strings\n",
    "    for category, search_strings in categories.items():\n",
    "        found = False  # Flag to track if any string is found\n",
    "        print(f\"------------------------------------------------\\n--> Searching for {category}...\\n\")\n",
    "        \n",
    "        for line_number, line in enumerate(content_lines, 1):\n",
    "            for string in search_strings:\n",
    "                if string in line:\n",
    "                    print(f\"      ✓✓ '{string}' mentioned on line {line_number}: {line.strip()}\\n\")\n",
    "                    found = True\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"      XX No evidence of {category} found :(\\n\")\n",
    "\n",
    "\n",
    "static_analysis(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4.1.6 Identify Application Entry Points Notes](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/06-Identify_Application_Entry_Points)\n",
    "\n",
    "- Identify possible entry and injection points through request and response analysis.\n",
    "- Use BurpSuite (Attack Surface Detector plugin)\n",
    "    - Interact with website and make notes of the parameters used in headers and URLs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4.1.7 Map Execution Paths Through Application Notes](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/07-Map_Execution_Paths_Through_Application)\n",
    "\n",
    "- Map the target application and understand the principal workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4.1.8 Fingerprint Web Application Framework Notes](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/08-Fingerprint_Web_Application_Framework)\n",
    "\n",
    "There are several common locations to consider in order to identify frameworks or components:\n",
    "- HTTP headers - performed in 4.1.2\n",
    "- Cookies - The name of the cookie (sometimes in header response) will indicate which framework is being used\n",
    "- HTML source code - handled in 4.1.5\n",
    "- Specific files and folders - handled in 4.1.3\n",
    "- File extensions - somewhat handled in 4.1.5\n",
    "- Error messages - error messages might be verbose enough to trigger a clue. 4.1.2 is designed to trigger an error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4.1.9 Fingerprint Web Application Notes](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/09-Fingerprint_Web_Application)\n",
    "- This content has been merged into: Fingerprint Web Application Framework. (4.1.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4.1.10 Map Application Architecture Notes](https://owasp.org/www-project-web-security-testing-guide/stable/4-Web_Application_Security_Testing/01-Information_Gathering/10-Map_Application_Architecture)\n",
    "- Generate a map of the application at hand based on the research conducted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
